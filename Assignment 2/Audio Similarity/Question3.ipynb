{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0d871f-957f-4314-94e3-16b9c1c4cf9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85832872-5393-4ea9-9f6c-6a49c94386ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/10/07 16:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>rsh224 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4044\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>rsh224 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>16</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>rsh224</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>8g</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1759806694191</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>8g</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1759806694285</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>64</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/rsh224/spark/</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>rsh224-notebook-866eb399bca7272f</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-b87155882a574cca9972145369a3cb49</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-09-21T20:54:03Z&se=2026-12-31T04:09:03Z&spr=https&sv=2024-11-04&sr=c&sig=5V91JeJe9mD%2FuPKUQ3LCErJh%2FwP0gNYoyl8MMx5pdkM%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>rsh224 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=4, worker_memory=8, master_memory=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787cdb5d-6b5a-4fe1-8cbe-5ef019d4537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from helpers import load_feature, stratified_split, apply_class_weights, get_multiclass_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c52349-b2c1-4ec5-805e-55f9f2fa7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/msd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98644061-3909-4ae1-a839-a2c891895ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = f'{username}/msd/output/feature_genre'\n",
    "input_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{features_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63510c6f-a7cd-4f33-8101-3fb0957556fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "features = spark.read.csv(\n",
    "    input_path,\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979d332-110a-4825-804e-543d00b8ce70",
   "metadata": {},
   "source": [
    "# Genre Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf4dbe4-9e40-4a55-8494-621953584501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol='genre', outputCol='label')\n",
    "\n",
    "features_indexed = indexer.fit(features).transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f43ef1-4e5c-46bd-bd32-214fe1b4064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "class_balance_df = features_indexed.groupBy('genre', 'label').count().orderBy('count', ascending=False)\n",
    "total = features_indexed.count()\n",
    "class_balance_df = class_balance_df.withColumn('proportion', F.round(F.col('count') / total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d886b0-103d-4d9d-b776-23a1f7559590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>  (20 + 1) / 21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+------+----------+\n",
      "|         genre|label| count|proportion|\n",
      "+--------------+-----+------+----------+\n",
      "|      Pop_Rock|  0.0|237641|     0.565|\n",
      "|    Electronic|  1.0| 40662|     0.097|\n",
      "|           Rap|  2.0| 20899|      0.05|\n",
      "|          Jazz|  3.0| 17774|     0.042|\n",
      "|         Latin|  4.0| 17503|     0.042|\n",
      "|           RnB|  5.0| 14314|     0.034|\n",
      "| International|  6.0| 14193|     0.034|\n",
      "|       Country|  7.0| 11691|     0.028|\n",
      "|     Religious|  8.0|  8779|     0.021|\n",
      "|        Reggae|  9.0|  6928|     0.016|\n",
      "|         Blues| 10.0|  6800|     0.016|\n",
      "|         Vocal| 11.0|  6182|     0.015|\n",
      "|          Folk| 12.0|  5789|     0.014|\n",
      "|       New Age| 13.0|  4000|      0.01|\n",
      "| Comedy_Spoken| 14.0|  2067|     0.005|\n",
      "|         Stage| 15.0|  1613|     0.004|\n",
      "|Easy_Listening| 16.0|  1535|     0.004|\n",
      "|   Avant_Garde| 17.0|  1012|     0.002|\n",
      "|     Classical| 18.0|   555|     0.001|\n",
      "|      Children| 19.0|   463|     0.001|\n",
      "+--------------+-----+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "class_balance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc61f6-d766-4aa5-8984-9ca74f8ce1ae",
   "metadata": {},
   "source": [
    "# Scaling and Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0810e30-60f7-476c-9b72-67f5ac053812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 582:==================================================>     (9 + 1) / 10]"
     ]
    }
   ],
   "source": [
    "feature_cols = [c for c in features_indexed.columns if c not in ('track_id', 'genre', 'label')]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_raw')\n",
    "assembled_features = assembler.transform(features_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35322664-8ef5-410c-a413-a175e30c4f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol='features_raw',\n",
    "    outputCol='features',\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaled_df = scaler.fit(assembled_features).transform(assembled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c848e86d-ea25-48c7-96a8-73050a952257",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = scaled_df.select('track_id', 'features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96f40e3b-e2c9-434f-baa9-f8a1f334b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "train, test = stratified_split(scaled_df, label_col='label', train_fraction=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50456d-597f-4bd7-a229-dc16163779cc",
   "metadata": {},
   "source": [
    "# Training without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a546753-e44b-491f-80b6-179c5ef329b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1007:===================================================>  (20 + 1) / 21]"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    family='multinomial',\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "model = lr.fit(train)\n",
    "predictions = model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36fca43e-209b-4c6f-84c2-bcdee4db7f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+\n",
      "|           accuracy|         precision|            recall|                f1|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "|0.41458304232434695|0.6768299947470359|0.4145830423243471|0.4768539237104466|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluations = get_multiclass_metrics(predictions)\n",
    "eval_df = spark.createDataFrame([Row(**evaluations)])\n",
    "eval_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69a32a-8840-4540-b774-9bf784e01696",
   "metadata": {},
   "source": [
    "# Training with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4147a931-d75b-4d87-ad0c-6748c4fe4b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "train_weighted = apply_class_weights(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63545227-a0b7-4c0a-9798-eac8b29971e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1366:===================================================>  (20 + 1) / 21]"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    weightCol='weight',\n",
    "    family='multinomial',\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "model = lr.fit(train_weighted)\n",
    "predictions = model.transform(train_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24fa0ab2-6e73-4f29-bcfd-f45d8d20fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+\n",
      "|           accuracy|         precision|            recall|                f1|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "|0.41458304232434695|0.6768299947470359|0.4145830423243471|0.4768539237104466|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluations = get_multiclass_metrics(predictions)\n",
    "eval_df = spark.createDataFrame([Row(**evaluations)])\n",
    "eval_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626d90b-9d61-4007-979f-4468cc5c069f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28276154-cebf-4ca4-a320-fcbf476b2ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0484b39-2b75-4af8-a1de-b1a7a2d19e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
