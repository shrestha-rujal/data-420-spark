{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame API ###\n",
    "\n",
    "The code below demonstrates some common **transformations**, **actions**, and **functions** in the DataFrame API.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Transformations](#Transformations)\n",
    "- [Null values](#Null-values)\n",
    "- [Statistics](#Statistics)\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The datasets used in these examples are designed to have as much complexity as possible while still being small.\n",
    "- The examples use `printSchema` and `show_as_html` frequently to show the contents of dataframes as they are transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import the Row object and the functions and types defined in the pyspark.sql module\n",
    "\n",
    "from pyspark.sql import Row, functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "This code creates two datasets, `data` and `department_data`.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- These datasets are designed to have as much complexity as possible while still being small.\n",
    "- The datasets are contructed in Python using pyspark Row objects, distributed to give an RDD, and then wrapped with a DataFrame = Dataset[Row].\n",
    "- This code does not load any data from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, distribute, and wrap data by hand\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\"       ,  StringType() , True),\n",
    "    StructField(\"Department\" ,  StringType() , True),\n",
    "    StructField(\"Age\"        , IntegerType() , True),\n",
    "    StructField(\"Gender\"     ,  StringType() , True),\n",
    "    StructField(\"Salary\"     ,  DoubleType() , True)\n",
    "])\n",
    "data = spark.createDataFrame(  # Finally, wrap the RDD with metadata by creating a DataFrame = Dataset[Row]\n",
    "    sc.parallelize(  # Second, take that list of pyspark row objects, distribute them as Spark rows in an RDD[Row]\n",
    "        [  # First, define a list of pyspark row objects (this is just a Python list in memory on the master node)\n",
    "            Row(\"Alpha One\"   , \"X\" , 28 , \"M\"  ,  80000.0),\n",
    "            Row(\"Bravo Two\"   , \"X\" , 25 , \"M\"  ,  70000.0),\n",
    "            Row(\"Charlie\"     , \"X\" , 23 , \"M\"  ,  80000.0),  # Charlie has no last name, duplicate salary in department X\n",
    "            Row(\"Delta Four\"  , \"Y\" , 30 , None , 100000.0),  # Gender is none\n",
    "            Row(\"Echo Five\"   , \"Y\" , 27 , \"F\"  , 120000.0),\n",
    "            Row(\"Foxtrot Six\" , \"Z\" , 20 , \"F\"  ,  90000.0),\n",
    "            Row(\"Golf Seven\"  , \"Z\" , 20 , \"F\"  ,  50000.0),  # Duplicate age in department Z\n",
    "            Row(\"Hotel Eight\" , \"Z\" , 38 , \"F\"  , 100000.0),\n",
    "            Row(\"Indigo Nine\" , \"Z\" , 50 , \"M\"  ,  70000.0),\n",
    "            Row(\"Juliet Ten\"  , \"Z\" , 18 , \"F\"  ,     None),  # Salary is none\n",
    "        ]\n",
    "    ), schema=schema)\n",
    "\n",
    "print(type(data))\n",
    "data.printSchema()\n",
    "print(data)\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, distribute, and wrap additional department data by hand\n",
    "\n",
    "department_schema = StructType([\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Campus\", StringType(), True)\n",
    "])\n",
    "department_data = spark.createDataFrame(\n",
    "    sc.parallelize(\n",
    "        [\n",
    "            Row(\"X\", \"Xray\",   \"U\"),\n",
    "            Row(\"Y\", \"Yankee\", \"V\"),\n",
    "            Row(\"Z\", \"Zulu\",   \"W\"),\n",
    "        ]\n",
    "    ), schema=department_schema)\n",
    "\n",
    "print(type(department_data))\n",
    "department_data.printSchema()\n",
    "print(department_data)\n",
    "show_as_html(department_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations ###\n",
    "\n",
    "The following are the most common untyped DataFrame transformations that you are likely to use regularly.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- Note that these cells do not modify `data` directly but rather define another variable e.g. `temp` so that each cell can be run in isolation successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column without change other columns\n",
    "\n",
    "temp = data.withColumnRenamed(\"Name\", \"FullName\")\n",
    "temp.printSchema()\n",
    "show_as_html(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column and automatically select existing columns (note that schema is automatically inferred)\n",
    "\n",
    "temp = data.withColumn(\"Names\", F.split(F.col(\"Name\"), \" \"))\n",
    "temp.printSchema()\n",
    "show_as_html(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a variety of columns or expressions involving columns (note use of F.col() to access methods such as .alias()) \n",
    "\n",
    "temp = data.select(\n",
    "    F.col(\"Name\").alias(\"FullName\"),\n",
    "    F.split(F.col(\"Name\"), \" \").alias(\"Names\"),\n",
    "    F.col(\"Department\"),\n",
    "    \"Age\",\n",
    "    \"Gender\",\n",
    "    \"Salary\",\n",
    ")\n",
    "temp.printSchema()\n",
    "show_as_html(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort on a subset of columns or column expressions\n",
    "\n",
    "temp = data.sort([\"Department\", \"Gender\", F.col(\"Salary\").desc()])\n",
    "temp.printSchema()\n",
    "show_as_html(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data on one or more columns using groupBy\n",
    "\n",
    "department_salary_summary = (\n",
    "    data\n",
    "    .groupBy([\"Department\"])  # GroupedDataFrame object where you need to run .agg() before you can e.g. run .show()\n",
    "    .agg(\n",
    "        F.count(F.col(\"Name\")).alias(\"Count\"),\n",
    "        F.sum(F.col(\"Salary\")).alias(\"TotalSalary\"),\n",
    "    )\n",
    "    .sort([\"Department\"])\n",
    ")\n",
    "department_salary_summary.printSchema()\n",
    "show_as_html(department_salary_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join data and department data (you could rename department data columns in the statement to avoid column name conflicts)\n",
    "\n",
    "data_with_department_name = (\n",
    "    data\n",
    "    .join(department_data, on=\"Department\", how=\"inner\")\n",
    "    .sort([\"Department\"])\n",
    ")\n",
    "data_with_department_name.printSchema()\n",
    "show_as_html(data_with_department_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values ###\n",
    "\n",
    "There is a submodule for transformations that involve null values. \n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The submodule also includes replacing not null values with other values as this is the same type of transformation anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's have another look at data before running each of the cells below\n",
    "\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "\n",
    "show_as_html(data.na.drop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values only on a subset of columns e.g. Salary\n",
    "\n",
    "show_as_html(data.na.drop(subset=[\"Salary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric null values with 0 or 0.0\n",
    "\n",
    "show_as_html(data.na.fill(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill string null values with \"Prefer not to say\"\n",
    "\n",
    "show_as_html(data.na.fill(\"Prefer not to say\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"X\" with \"Y\" in the Department column only\n",
    "\n",
    "show_as_html(data.na.replace(\"X\", \"Y\", subset=[\"Department\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics ###\n",
    "\n",
    "There is a submodule for computing statistics and doing simple random sampling.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The submodule does not provide exact stratified random sampling, we will look at this in later examples when we talk about sampling for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's have another look at data before running each of the cells below\n",
    "\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple random sampling per Department with specific sampling probabilities\n",
    "\n",
    "show_as_html(data.stat.sampleBy(\"Department\", {\"X\": 0.5, \"Y\": 0.5, \"Z\": 0.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate quantiles with different relative errors (note that a relative error of 0.0 gives you the exact quantiles)\n",
    "\n",
    "probabilities = [0.0, 0.25, 0.5, 0.75, 1.0]  # [min, Q1, median, Q3, max]\n",
    "\n",
    "relativeError = 0.1\n",
    "\n",
    "approxQuantiles = data.stat.approxQuantile(\"Salary\", probabilities, relativeError)\n",
    "\n",
    "print([row[\"Salary\"] for row in data.toLocalIterator()])\n",
    "print(\"\")\n",
    "print(approxQuantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can automate this to compare results for different relative errors \n",
    "\n",
    "probabilities = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "results = []\n",
    "for relativeError in [1.0, 0.5, 0.1, 0.0]:\n",
    "    approxQuantiles = data.stat.approxQuantile(\"Salary\", probabilities, relativeError)\n",
    "    results.append([relativeError] + approxQuantiles)\n",
    "\n",
    "display(pandas.DataFrame(results, columns=[\"relativeError\"] + probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
