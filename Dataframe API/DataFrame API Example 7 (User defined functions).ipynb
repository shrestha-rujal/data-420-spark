{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame API ###\n",
    "\n",
    "The code below demonstrates some common **transformations**, **actions**, and **functions** in the DataFrame API.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- [Data](#Data)\n",
    "- [User defined functions](#User-defined-functions)\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The datasets used in these examples are designed to have as much complexity as possible while still being small.\n",
    "- The examples use `printSchema` and `show_as_html` frequently to show the contents of dataframes as they are transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/08/16 22:18:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>rsh224 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4041\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>rsh224 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>rsh224</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>rsh224-notebook-ee376b98b2630e1e</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/rsh224/spark/</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-e5149934baaa47a69a6fe7ed503a67e9</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1755339492130</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1755339491960</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>rsh224 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need to import the Row object and the pyspark sql types, everything else we get from the global variables sc or spark\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "This code creates two datasets, `data` and `department_data`.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- These datasets are designed to have as much complexity as possible while still being small.\n",
    "- The datasets are contructed in Python using pyspark Row objects, distributed to give an RDD, and then wrapped with a DataFrame = Dataset[Row].\n",
    "- This code does not load any data from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      "\n",
      "DataFrame[Name: string, Department: string, Age: int, Gender: string, Salary: double]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Department</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alpha One</td>\n",
       "      <td>X</td>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bravo Two</td>\n",
       "      <td>X</td>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>X</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delta Four</td>\n",
       "      <td>Y</td>\n",
       "      <td>30</td>\n",
       "      <td>None</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Echo Five</td>\n",
       "      <td>Y</td>\n",
       "      <td>27</td>\n",
       "      <td>F</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Foxtrot Six</td>\n",
       "      <td>Z</td>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Golf Seven</td>\n",
       "      <td>Z</td>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hotel Eight</td>\n",
       "      <td>Z</td>\n",
       "      <td>38</td>\n",
       "      <td>F</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indigo Nine</td>\n",
       "      <td>Z</td>\n",
       "      <td>50</td>\n",
       "      <td>M</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Juliet Ten</td>\n",
       "      <td>Z</td>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name Department  Age Gender    Salary\n",
       "0    Alpha One          X   28      M   80000.0\n",
       "1    Bravo Two          X   25      M   70000.0\n",
       "2      Charlie          X   23      M   80000.0\n",
       "3   Delta Four          Y   30   None  100000.0\n",
       "4    Echo Five          Y   27      F  120000.0\n",
       "5  Foxtrot Six          Z   20      F   90000.0\n",
       "6   Golf Seven          Z   20      F   50000.0\n",
       "7  Hotel Eight          Z   38      F  100000.0\n",
       "8  Indigo Nine          Z   50      M   70000.0\n",
       "9   Juliet Ten          Z   18      F       NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create, distribute, and wrap data by hand\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\"       ,  StringType() , True),\n",
    "    StructField(\"Department\" ,  StringType() , True),\n",
    "    StructField(\"Age\"        , IntegerType() , True),\n",
    "    StructField(\"Gender\"     ,  StringType() , True),\n",
    "    StructField(\"Salary\"     ,  DoubleType() , True)\n",
    "])\n",
    "data = spark.createDataFrame(  # Finally, wrap the RDD with metadata by creating a DataFrame = Dataset[Row]\n",
    "    sc.parallelize(  # Second, take that list of pyspark row objects, distribute them as Spark rows in an RDD[Row]\n",
    "        [  # First, define a list of pyspark row objects (this is just a Python list in memory on the master node)\n",
    "            Row(\"Alpha One\"   , \"X\" , 28 , \"M\"  ,  80000.0),\n",
    "            Row(\"Bravo Two\"   , \"X\" , 25 , \"M\"  ,  70000.0),\n",
    "            Row(\"Charlie\"     , \"X\" , 23 , \"M\"  ,  80000.0),  # Charlie has no last name, duplicate salary in department X\n",
    "            Row(\"Delta Four\"  , \"Y\" , 30 , None , 100000.0),  # Gender is none\n",
    "            Row(\"Echo Five\"   , \"Y\" , 27 , \"F\"  , 120000.0),\n",
    "            Row(\"Foxtrot Six\" , \"Z\" , 20 , \"F\"  ,  90000.0),\n",
    "            Row(\"Golf Seven\"  , \"Z\" , 20 , \"F\"  ,  50000.0),  # Duplicate age in department Z\n",
    "            Row(\"Hotel Eight\" , \"Z\" , 38 , \"F\"  , 100000.0),\n",
    "            Row(\"Indigo Nine\" , \"Z\" , 50 , \"M\"  ,  70000.0),\n",
    "            Row(\"Juliet Ten\"  , \"Z\" , 18 , \"F\"  ,     None),  # Salary is none\n",
    "        ]\n",
    "    ), schema=schema)\n",
    "\n",
    "print(type(data))\n",
    "data.printSchema()\n",
    "print(data)\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Campus: string (nullable = true)\n",
      "\n",
      "DataFrame[Department: string, Name: string, Campus: string]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Name</th>\n",
       "      <th>Campus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X</td>\n",
       "      <td>Xray</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>Yankee</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Department    Name Campus\n",
       "0          X    Xray      U\n",
       "1          Y  Yankee      V\n",
       "2          Z    Zulu      W"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create, distribute, and wrap additional department data by hand\n",
    "\n",
    "department_schema = StructType([\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Campus\", StringType(), True)\n",
    "])\n",
    "department_data = spark.createDataFrame(\n",
    "    sc.parallelize(\n",
    "        [\n",
    "            Row(\"X\", \"Xray\",   \"U\"),\n",
    "            Row(\"Y\", \"Yankee\", \"V\"),\n",
    "            Row(\"Z\", \"Zulu\",   \"W\"),\n",
    "        ]\n",
    "    ), schema=department_schema)\n",
    "\n",
    "print(type(department_data))\n",
    "department_data.printSchema()\n",
    "print(department_data)\n",
    "show_as_html(department_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions ###\n",
    "\n",
    "The following user defined functions allow you to use arbitrary Python logic to operate on each row in a dataframe in a distributed way.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- A user defined function can be any Python function provided that function is portable or any modules used are installed on the worker nodes as well.\n",
    "- The function is wrapped in `F.udf` to convert it to a pyspark column transformation that can be understood by Spark.\n",
    "- The pyspark return type of the function needs to be specified so that Spark knows how to convert or structure what is returned.\n",
    "- You lose all optimizations that Spark can apply such as filtering on filescan instead of in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha One\n",
      "Charlie None\n"
     ]
    }
   ],
   "source": [
    "# Python lambda functions, nothing to do with Spark\n",
    "\n",
    "my_full_name = lambda x, y: f\"{x} {y}\"\n",
    "\n",
    "print(my_full_name(\"Alpha\", \"One\"))\n",
    "print(my_full_name(\"Charlie\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha One\n",
      "Alpha One\n",
      "One, Alpha\n",
      "Charlie\n",
      "Charlie\n",
      "Charlie\n"
     ]
    }
   ],
   "source": [
    "# Python def function instead, nothing to do with Spark\n",
    "\n",
    "def my_full_name_formatted(first_name, last_name, style=\"traditional\"):\n",
    "    \"\"\"Combine first and last names into a full name with optional formatting.\n",
    "    \n",
    "    Args:\n",
    "        first_name (str): first name\n",
    "        last_name (str): last name\n",
    "        style (str): formatting to apply, either \"traditional\" or \"formal\"\n",
    "        \n",
    "    Returns:\n",
    "        full_name (str): first and last name combined\n",
    "    \"\"\"\n",
    "    \n",
    "    if last_name is None:\n",
    "        return first_name\n",
    "\n",
    "    if first_name is None:\n",
    "        return last_name\n",
    "\n",
    "    if style == \"traditional\":\n",
    "        return first_name + \" \" + last_name\n",
    "    elif style == \"formal\":\n",
    "        return f\"{last_name}, {first_name}\"\n",
    "    else:\n",
    "        return first_name + \" \" + last_name\n",
    "\n",
    "\n",
    "print(my_full_name_formatted(\"Alpha\", \"One\"))\n",
    "print(my_full_name_formatted(\"Alpha\", \"One\", style=\"traditional\"))\n",
    "print(my_full_name_formatted(\"Alpha\", \"One\", style=\"formal\"))\n",
    "\n",
    "print(my_full_name_formatted(\"Charlie\", None))\n",
    "print(my_full_name_formatted(\"Charlie\", None, style=\"traditional\"))\n",
    "print(my_full_name_formatted(\"Charlie\", None, style=\"formal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha One\n",
      "One, Alpha\n",
      "Charlie\n",
      "Charlie\n"
     ]
    }
   ],
   "source": [
    "# Wrap functions to override default arguments, nothing to do with Spark\n",
    "\n",
    "my_full_name_traditional = lambda x, y: my_full_name_formatted(x, y, style=\"traditional\")\n",
    "my_full_name_formal = lambda x, y: my_full_name_formatted(x, y, style=\"formal\")\n",
    "\n",
    "print(my_full_name_traditional(\"Alpha\", \"One\"))\n",
    "print(my_full_name_formal(\"Alpha\", \"One\"))\n",
    "\n",
    "print(my_full_name_traditional(\"Charlie\", None))\n",
    "print(my_full_name_formal(\"Charlie\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- FullNameTraditional: string (nullable = true)\n",
      " |-- FullNameFormal: string (nullable = true)\n",
      " |-- StructOutput: struct (nullable = true)\n",
      " |    |-- A: integer (nullable = true)\n",
      " |    |-- B: integer (nullable = true)\n",
      " |-- ArrayOutput: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- StructOutput.A: integer (nullable = true)\n",
      " |-- StructOutput.B: integer (nullable = true)\n",
      " |-- ArrayOutput[0]: integer (nullable = true)\n",
      " |-- ArrayOutput[1]: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullName</th>\n",
       "      <th>FullNameTraditional</th>\n",
       "      <th>FullNameFormal</th>\n",
       "      <th>StructOutput</th>\n",
       "      <th>ArrayOutput</th>\n",
       "      <th>StructOutput.A</th>\n",
       "      <th>StructOutput.B</th>\n",
       "      <th>ArrayOutput[0]</th>\n",
       "      <th>ArrayOutput[1]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alpha One</td>\n",
       "      <td>Alpha One</td>\n",
       "      <td>One, Alpha</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bravo Two</td>\n",
       "      <td>Bravo Two</td>\n",
       "      <td>Two, Bravo</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie None</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delta Four</td>\n",
       "      <td>Delta Four</td>\n",
       "      <td>Four, Delta</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Echo Five</td>\n",
       "      <td>Echo Five</td>\n",
       "      <td>Five, Echo</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Foxtrot Six</td>\n",
       "      <td>Foxtrot Six</td>\n",
       "      <td>Six, Foxtrot</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Golf Seven</td>\n",
       "      <td>Golf Seven</td>\n",
       "      <td>Seven, Golf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hotel Eight</td>\n",
       "      <td>Hotel Eight</td>\n",
       "      <td>Eight, Hotel</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indigo Nine</td>\n",
       "      <td>Indigo Nine</td>\n",
       "      <td>Nine, Indigo</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Juliet Ten</td>\n",
       "      <td>Juliet Ten</td>\n",
       "      <td>Ten, Juliet</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FullName FullNameTraditional FullNameFormal StructOutput  \\\n",
       "0     Alpha One           Alpha One     One, Alpha       (1, 2)   \n",
       "1     Bravo Two           Bravo Two     Two, Bravo       (1, 2)   \n",
       "2  Charlie None             Charlie        Charlie       (1, 2)   \n",
       "3    Delta Four          Delta Four    Four, Delta       (1, 2)   \n",
       "4     Echo Five           Echo Five     Five, Echo       (1, 2)   \n",
       "5   Foxtrot Six         Foxtrot Six   Six, Foxtrot       (1, 2)   \n",
       "6    Golf Seven          Golf Seven    Seven, Golf       (1, 2)   \n",
       "7   Hotel Eight         Hotel Eight   Eight, Hotel       (1, 2)   \n",
       "8   Indigo Nine         Indigo Nine   Nine, Indigo       (1, 2)   \n",
       "9    Juliet Ten          Juliet Ten    Ten, Juliet       (1, 2)   \n",
       "\n",
       "       ArrayOutput  StructOutput.A  StructOutput.B  ArrayOutput[0]  \\\n",
       "0  [1, 2, 3, 4, 5]               1               2               1   \n",
       "1  [1, 2, 3, 4, 5]               1               2               1   \n",
       "2  [1, 2, 3, 4, 5]               1               2               1   \n",
       "3  [1, 2, 3, 4, 5]               1               2               1   \n",
       "4  [1, 2, 3, 4, 5]               1               2               1   \n",
       "5  [1, 2, 3, 4, 5]               1               2               1   \n",
       "6  [1, 2, 3, 4, 5]               1               2               1   \n",
       "7  [1, 2, 3, 4, 5]               1               2               1   \n",
       "8  [1, 2, 3, 4, 5]               1               2               1   \n",
       "9  [1, 2, 3, 4, 5]               1               2               1   \n",
       "\n",
       "   ArrayOutput[1]  \n",
       "0               2  \n",
       "1               2  \n",
       "2               2  \n",
       "3               2  \n",
       "4               2  \n",
       "5               2  \n",
       "6               2  \n",
       "7               2  \n",
       "8               2  \n",
       "9               2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pyspark user defined functions\n",
    "\n",
    "my_full_name_udf = F.udf(my_full_name, StringType())\n",
    "my_full_name_traditional_udf = F.udf(my_full_name_traditional, StringType())\n",
    "my_full_name_formal_udf = F.udf(my_full_name_formal, StringType())\n",
    "my_struct_udf = F.udf(\n",
    "    lambda: {\"B\": 2, \"A\": 1},\n",
    "    StructType([\n",
    "        StructField(\"A\", IntegerType()),\n",
    "        StructField(\"B\", IntegerType()),\n",
    "    ])\n",
    ")\n",
    "my_array_udf = F.udf(\n",
    "    lambda: [1, 2, 3, 4, 5],\n",
    "    ArrayType(IntegerType()),\n",
    ")\n",
    "\n",
    "temp = (\n",
    "    data\n",
    "    .withColumn(\"Names\", F.split(F.col(\"Name\"), \" \"))\n",
    "    .select(\n",
    "        F.col(\"Names\")[0].alias(\"FirstName\"),\n",
    "        F.col(\"Names\")[1].alias(\"LastName\"),\n",
    "    )\n",
    "    .select(\n",
    "        my_full_name_udf(F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"FullName\"),\n",
    "        my_full_name_traditional_udf(F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"FullNameTraditional\"),\n",
    "        my_full_name_formal_udf(F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"FullNameFormal\"),\n",
    "        my_struct_udf().alias(\"StructOutput\"),\n",
    "        my_array_udf().alias(\"ArrayOutput\"),\n",
    "    )\n",
    "    .withColumn(\"StructOutput.A\", F.col(\"StructOutput\")[\"A\"])\n",
    "    .withColumn(\"StructOutput.B\", F.col(\"StructOutput\")[\"B\"])\n",
    "    .withColumn(\"ArrayOutput[0]\", F.col(\"ArrayOutput\")[0])\n",
    "    .withColumn(\"ArrayOutput[1]\", F.col(\"ArrayOutput\")[1])\n",
    ")\n",
    "temp.printSchema()\n",
    "show_as_html(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/16 22:48:18 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>rsh224 (notebook)</code> is under the completed applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
