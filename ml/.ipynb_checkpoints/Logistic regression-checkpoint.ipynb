{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning in Spark ###\n",
    "\n",
    "The code below trains a logistic regression model in Spark.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Feature engineering](#Feature-engineering)\n",
    "- [Training](#Training)\n",
    "- [Evaluating](#Evaluating)\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data below is transcribed from the `mtcars` dataset in R.\n",
    "- The classification metrics are generated using multiple `.count()` actions and would be inefficient unless `pred` is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import pyplot from matplotlib in order to visualize our data locally \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import Row, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "This code creates the `cars` dataset that comes bundled with R.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The datasets are contructed in Python using pyspark Row objects, distributed to give an RDD, and then wrapped with a DataFrame = Dataset[Row].\n",
    "- This code does not load any data from HDFS.\n",
    "- The data is small so we can use `.toPandas` freely to collect and visualize the data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, distributed, and wrap data by hand\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"model\", StringType(), True),\n",
    "  StructField(\"mpg\", DoubleType(), True),\n",
    "  StructField(\"cyl\", IntegerType(), True),\n",
    "  StructField(\"disp\", DoubleType(), True),\n",
    "  StructField(\"hp\", DoubleType(), True),\n",
    "  StructField(\"drat\", DoubleType(), True),\n",
    "  StructField(\"wt\", DoubleType(), True),\n",
    "  StructField(\"qsec\", DoubleType(), True),\n",
    "  StructField(\"vs\", IntegerType(), True),\n",
    "  StructField(\"am\", IntegerType(), True),\n",
    "  StructField(\"gear\", IntegerType(), True),\n",
    "  StructField(\"carb\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "cars = spark.createDataFrame(  # Finally, wrap the RDD in a DataFrame = Dataset[Row]\n",
    "  sc.parallelize(  # Second, take that list of pyspark row objects, distribute them as Spark rows in an RDD[Row]\n",
    "    [  # First, define a Python list of pyspark row objects (this is just a Python list in master memory)\n",
    "      Row(\"Mazda RX4\",           21.0, 6, 160.0, 110.0, 3.90, 2.620, 16.46, 0, 1, 4, 4),\n",
    "      Row(\"Mazda RX4 Wag\",       21.0, 6, 160.0, 110.0, 3.90, 2.875, 17.02, 0, 1, 4, 4),\n",
    "      Row(\"Datsun 710\",          22.8, 4, 108.0,  93.0, 3.85, 2.320, 18.61, 1, 1, 4, 1),\n",
    "      Row(\"Hornet 4 Drive\",      21.4, 6, 258.0, 110.0, 3.08, 3.215, 19.44, 1, 0, 3, 1),\n",
    "      Row(\"Hornet Sportabout\",   18.7, 8, 360.0, 175.0, 3.15, 3.440, 17.02, 0, 0, 3, 2),\n",
    "      Row(\"Valiant\",             18.1, 6, 225.0, 105.0, 2.76, 3.460, 20.22, 1, 0, 3, 1),\n",
    "      Row(\"Duster 360\",          14.3, 8, 360.0, 245.0, 3.21, 3.570, 15.84, 0, 0, 3, 4),\n",
    "      Row(\"Merc 240D\",           24.4, 4, 146.7,  62.0, 3.69, 3.190, 20.00, 1, 0, 4, 2),\n",
    "      Row(\"Merc 230\",            22.8, 4, 140.8,  95.0, 3.92, 3.150, 22.90, 1, 0, 4, 2),\n",
    "      Row(\"Merc 280\",            19.2, 6, 167.6, 123.0, 3.92, 3.440, 18.30, 1, 0, 4, 4),\n",
    "      Row(\"Merc 280C\",           17.8, 6, 167.6, 123.0, 3.92, 3.440, 18.90, 1, 0, 4, 4),\n",
    "      Row(\"Merc 450SE\",          16.4, 8, 275.8, 180.0, 3.07, 4.070, 17.40, 0, 0, 3, 3),\n",
    "      Row(\"Merc 450SL\",          17.3, 8, 275.8, 180.0, 3.07, 3.730, 17.60, 0, 0, 3, 3),\n",
    "      Row(\"Merc 450SLC\",         15.2, 8, 275.8, 180.0, 3.07, 3.780, 18.00, 0, 0, 3, 3),\n",
    "      Row(\"Cadillac Fleetwood\",  10.4, 8, 472.0, 205.0, 2.93, 5.250, 17.98, 0, 0, 3, 4),\n",
    "      Row(\"Lincoln Continental\", 10.4, 8, 460.0, 215.0, 3.00, 5.424, 17.82, 0, 0, 3, 4),\n",
    "      Row(\"Chrysler Imperial\",   14.7, 8, 440.0, 230.0, 3.23, 5.345, 17.42, 0, 0, 3, 4),\n",
    "      Row(\"Fiat 128\",            32.4, 4,  78.7,  66.0, 4.08, 2.200, 19.47, 1, 1, 4, 1),\n",
    "      Row(\"Honda Civic\",         30.4, 4,  75.7,  52.0, 4.93, 1.615, 18.52, 1, 1, 4, 2),\n",
    "      Row(\"Toyota Corolla\",      33.9, 4,  71.1,  65.0, 4.22, 1.835, 19.90, 1, 1, 4, 1),\n",
    "      Row(\"Toyota Corona\",       21.5, 4, 120.1,  97.0, 3.70, 2.465, 20.01, 1, 0, 3, 1),\n",
    "      Row(\"Dodge Challenger\",    15.5, 8, 318.0, 150.0, 2.76, 3.520, 16.87, 0, 0, 3, 2),\n",
    "      Row(\"AMC Javelin\",         15.2, 8, 304.0, 150.0, 3.15, 3.435, 17.30, 0, 0, 3, 2),\n",
    "      Row(\"Camaro Z28\",          13.3, 8, 350.0, 245.0, 3.73, 3.840, 15.41, 0, 0, 3, 4),\n",
    "      Row(\"Pontiac Firebird\",    19.2, 8, 400.0, 175.0, 3.08, 3.845, 17.05, 0, 0, 3, 2),\n",
    "      Row(\"Fiat X1-9\",           27.3, 4,  79.0,  66.0, 4.08, 1.935, 18.90, 1, 1, 4, 1),\n",
    "      Row(\"Porsche 914-2\",       26.0, 4, 120.3,  91.0, 4.43, 2.140, 16.70, 0, 1, 5, 2),\n",
    "      Row(\"Lotus Europa\",        30.4, 4,  95.1, 113.0, 3.77, 1.513, 16.90, 1, 1, 5, 2),\n",
    "      Row(\"Ford Pantera L\",      15.8, 8, 351.0, 264.0, 4.22, 3.170, 14.50, 0, 1, 5, 4),\n",
    "      Row(\"Ferrari Dino\",        19.7, 6, 145.0, 175.0, 3.62, 2.770, 15.50, 0, 1, 5, 6),\n",
    "      Row(\"Maserati Bora\",       15.0, 8, 301.0, 335.0, 3.54, 3.570, 14.60, 0, 1, 5, 8),\n",
    "      Row(\"Volvo 142E\",          21.4, 4, 121.0, 109.0, 4.11, 2.780, 18.60, 1, 1, 4, 2),\n",
    "    ]\n",
    "  ), schema=schema)\n",
    "\n",
    "cars.cache()\n",
    "\n",
    "cars.printSchema()\n",
    "show_as_html(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty way to visualize the variables and their relationships\n",
    "\n",
    "cars_local = cars.toPandas()\n",
    "\n",
    "axes = pd.plotting.scatter_matrix(cars_local, figsize=(7, 7), range_padding=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "\n",
    "data = cars.select([\"vs\", \"mpg\"])\n",
    "\n",
    "data.printSchema()\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slighty more complicated visualization of engine type vs mpg\n",
    "\n",
    "data_local = data.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4), dpi=100)\n",
    "\n",
    "ax.plot(data_local['mpg'], data_local['vs'], 'xb')\n",
    "\n",
    "ax.margins(0.20, 0.50)\n",
    "ax.grid(color='lightgrey', linestyle='--')\n",
    "ax.set_title('Scatterplot of engine type vs mpg')\n",
    "ax.set_xlabel('mpg')\n",
    "ax.set_ylabel('vs')\n",
    "ax.set_yticks([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering ###\n",
    "\n",
    "In order to train our regression model, we need to assemble a vector of **numeric** features in a single column using `VectorAssembler`.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- `VectorAssembler` takes the column names as input, you do not need to use `F.col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding\n",
    "\n",
    "data = data.withColumn(\"label\", F.col(\"vs\").cast(IntegerType()))\n",
    "\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=[\"mpg\"], outputCol=\"features\")\n",
    "data = vector_assembler.transform(data)\n",
    "\n",
    "data.printSchema()\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ###\n",
    "\n",
    "First we need to construct an instance of the `LogisticRegression` class with the hyperparameters that we need.\n",
    "\n",
    "Then we fit our model using that instance as configured using `.fit()` to obtain a model class that we can use to `.transform()` our data to generate predictions. \n",
    "\n",
    "**Key points**\n",
    "\n",
    "- `LogisticRegression` is a class in the `pyspark.ml` submodule that needs to be configured before it can be used.\n",
    "- `lr` is the configured instance of that class that we can use to fit our model.\n",
    "- `lr_model` is the actual trained model that we can use to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0.0, featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(data)\n",
    "\n",
    "# Predict over training\n",
    "\n",
    "pred = lr_model.transform(data)\n",
    "pred.cache()\n",
    "\n",
    "pred.printSchema()\n",
    "show_as_html(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating ###\n",
    "\n",
    "Next we can evaluate the predictions from our model by collecting and visualizing them using `matplotlib` and by computing relevant classification metrics either by hand or by using an instance of `BinaryClassificationEvaluator` that we have configured.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- `BinaryClassificationEvaluator` is also a class in the `pyspark.ml` submodule that needs to be configured to compute a specific metric before it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect local ready for plotting\n",
    "\n",
    "pred_local = pred.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept and the coefficients from the model for visualizing\n",
    "\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "# Define the logistic function using the intercept and the coefficients\n",
    "\n",
    "def logistic_function(x, intercept, coefficients):\n",
    "    return 1 / (1 + np.exp(-(coefficients * x + intercept)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4), dpi=100)\n",
    "\n",
    "inflection_x = - intercept / coefficients[0]\n",
    "inflection_y = logistic_function(inflection_x, intercept, coefficients)\n",
    "x = np.linspace(pred_local['mpg'].min(), pred_local['mpg'].max(), 1000)\n",
    "y = logistic_function(x, intercept, coefficients)\n",
    "\n",
    "ax.plot(x, y, '-r')\n",
    "ax.plot(inflection_x, inflection_y, 'or', markersize=5)\n",
    "ax.axvline(inflection_x, linestyle='--', color='red')\n",
    "\n",
    "mask = pred_local['prediction'] == 0\n",
    "ax.plot(pred_local.loc[mask, 'mpg'], pred_local.loc[mask, 'vs'], 'xb')\n",
    "\n",
    "mask = pred_local['prediction'] == 1\n",
    "ax.plot(pred_local.loc[mask, 'mpg'], pred_local.loc[mask, 'vs'], 'xr')\n",
    "\n",
    "ax.margins(0.20, 0.50)\n",
    "ax.grid(color='lightgrey', linestyle='--')\n",
    "ax.set_title('Scatterplot of engine type vs mpg with logistic regression model')\n",
    "ax.set_xlabel('mpg')\n",
    "ax.set_ylabel('vs')\n",
    "ax.set_yticks([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4), dpi=100)\n",
    "\n",
    "inflection_x = - intercept / coefficients[0]\n",
    "inflection_y = logistic_function(inflection_x, intercept, coefficients)\n",
    "x = np.linspace(inflection_x - 50, inflection_x + 50, 1000)\n",
    "y = logistic_function(x, intercept, coefficients)\n",
    "\n",
    "ax.plot(x, y, '-r')\n",
    "ax.plot(inflection_x, inflection_y, 'or', markersize=5)\n",
    "ax.axvline(inflection_x, linestyle='--', color='red')\n",
    "\n",
    "mask = pred_local['prediction'] == 0\n",
    "ax.plot(pred_local.loc[mask, 'mpg'], pred_local.loc[mask, 'vs'], 'xb')\n",
    "\n",
    "mask = pred_local['prediction'] == 1\n",
    "ax.plot(pred_local.loc[mask, 'mpg'], pred_local.loc[mask, 'vs'], 'xr')\n",
    "\n",
    "ax.margins(0.20, 0.50)\n",
    "ax.grid(color='lightgrey', linestyle='--')\n",
    "ax.set_title('Scatterplot of engine type vs mpg with logistic regression model')\n",
    "ax.set_xlabel('mpg')\n",
    "ax.set_ylabel('vs')\n",
    "ax.set_yticks([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "total = pred.count()\n",
    "\n",
    "nP_actual = pred.filter((F.col('label') == 1)).count()\n",
    "nN_actual = pred.filter((F.col('label') == 0)).count()\n",
    "\n",
    "nP = pred.filter((F.col('prediction') == 1)).count()\n",
    "nN = pred.filter((F.col('prediction') == 0)).count()\n",
    "TP = pred.filter((F.col('prediction') == 1) & (F.col('label') == 1)).count()\n",
    "FP = pred.filter((F.col('prediction') == 1) & (F.col('label') == 0)).count()\n",
    "FN = pred.filter((F.col('prediction') == 0) & (F.col('label') == 1)).count()\n",
    "TN = pred.filter((F.col('prediction') == 0) & (F.col('label') == 0)).count()\n",
    "\n",
    "accuracy = (TP + TN) / total\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='label', metricName='areaUnderROC')\n",
    "auroc = binary_evaluator.evaluate(pred)\n",
    "\n",
    "print(f'Classification metrics for our model')\n",
    "print(f'')\n",
    "print(f'total:     {total}')\n",
    "print(f'')\n",
    "print(f'nP actual: {nP_actual}')\n",
    "print(f'nN actual: {nN_actual}')\n",
    "print(f'')\n",
    "print(f'nP:        {nP}')\n",
    "print(f'nN:        {nN}')\n",
    "print(f'')\n",
    "print(f'TP         {TP}')\n",
    "print(f'FP         {FP}')\n",
    "print(f'FN         {FN}')\n",
    "print(f'TN         {TN}')\n",
    "print(f'')\n",
    "print(f'accuracy:  {accuracy:.4f}')\n",
    "print(f'precision: {precision:.4f}')\n",
    "print(f'recall:    {recall:.4f}')\n",
    "print(f'')\n",
    "print(f'auroc:     {auroc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
