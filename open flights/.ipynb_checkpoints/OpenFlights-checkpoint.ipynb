{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenFlights example using the SQL and DataFrame API ###\n",
    "\n",
    "The code below shows you how to carry out more complex data analysis tasks using the SQL and DataFrame API.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- Make your code as readable as possible using multiline commands and indenting. You should look up Python style guides to improve your code readability.\n",
    "- You should always define schemas if you are loading a structured dataset with columns and data types.\n",
    "- The `show_as_html` command leverages existing pandas jupyter integration to show a limited number of rows of a spark dataframe as html.\n",
    "- You should always comment your code so that it can be read and understood by someone else (or by yourself several months from now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need to import pypsark.sql functions and types, everything else we get from the global variables sc or spark\n",
    "\n",
    "from pyspark.sql import functions as F  # this imports e.g. F.min, F.max, ... to avoid naming conflicts with Python min, max, ...\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "The OpenFlights data is an open source database of airports, train stations, and ferry terminals from around the world, including information about routes, airlines, airports, countries, and planes. You can find out more about the OpenFlights data on their website https://openflights.org/data.php.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- We we use the routes and airports datasets in this notebook. \n",
    "- We want to define schemas and load the data all at once so that it can be used consistently in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routes\n",
    "\n",
    "schema_routes = StructType([\n",
    "    StructField(\"airline\", StringType(), True),\n",
    "    StructField(\"airline_id\", StringType(), True),\n",
    "    StructField(\"src_airport\", StringType(), True),\n",
    "    StructField(\"src_airport_id\", StringType(), True),\n",
    "    StructField(\"dst_airport\", StringType(), True),\n",
    "    StructField(\"dst_airport_id\", StringType(), True),\n",
    "    StructField(\"codeshare\", StringType(), True),\n",
    "    StructField(\"stops\", IntegerType(), True),\n",
    "    StructField(\"equipment\", StringType(), True),\n",
    "])\n",
    "routes = (\n",
    "    spark.read.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .schema(schema_routes)\n",
    "    .load(\"hdfs:///data/openflights/routes.dat\")\n",
    ")\n",
    "print(routes)\n",
    "routes.printSchema()\n",
    "show_as_html(routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airports\n",
    "\n",
    "schema_airports = StructType([\n",
    "  StructField(\"airport_id\", IntegerType(), True),\n",
    "  StructField(\"airport\", StringType(), True),\n",
    "  StructField(\"city\", StringType(), True),\n",
    "  StructField(\"country\", StringType(), True),\n",
    "  StructField(\"iata\", StringType(), True),\n",
    "  StructField(\"icao\", StringType(), True),\n",
    "  StructField(\"latitude\", DoubleType(), True),\n",
    "  StructField(\"longitude\", DoubleType(), True),\n",
    "  StructField(\"altitude\", DoubleType(), True),\n",
    "  StructField(\"timezone\", DoubleType(), True),\n",
    "  StructField(\"dst\", StringType(), True),\n",
    "  StructField(\"tz\", StringType(), True),\n",
    "  StructField(\"type\", StringType(), True),\n",
    "  StructField(\"source\", StringType(), True),\n",
    "])\n",
    "airports = (\n",
    "    spark.read.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .schema(schema_airports)\n",
    "    .load(\"hdfs:////data/openflights/airports.dat\")\n",
    ")\n",
    "print(airports)\n",
    "airports.printSchema()\n",
    "show_as_html(airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL API ###\n",
    "\n",
    "In the cells below we use the SQL API to compute airport size based on the number of destination airports you can fly to from each airport. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute airport size using SQL API\n",
    "\n",
    "routes.registerTempTable('routes')\n",
    "airports.registerTempTable('airports')\n",
    "\n",
    "counts_sql = \"\"\"\n",
    "    SELECT\n",
    "        airports.airport AS airport_name,\n",
    "        airports.city AS airport_city,\n",
    "        airports.country AS airport_country,\n",
    "        counts.airport_size AS airport_size\n",
    "    FROM\n",
    "    (\n",
    "        SELECT\n",
    "            routes.src_airport_id AS airport_id,\n",
    "            count(routes.dst_airport_id) AS airport_size\n",
    "        FROM\n",
    "            routes\n",
    "        GROUP BY\n",
    "            airport_id\n",
    "    ) counts\n",
    "    LEFT JOIN\n",
    "        airports ON counts.airport_id = airports.airport_id\n",
    "    ORDER BY\n",
    "        airport_size DESC\n",
    "\"\"\"\n",
    "counts = spark.sql(counts_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)\n",
    "counts.printSchema()\n",
    "show_as_html(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame API ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple**\n",
    "\n",
    "- Define one variable representing the output of all transformations using a single multiline command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute airport size using DataFrame API\n",
    "\n",
    "counts = (\n",
    "    routes\n",
    "    # Select only the columns that are needed\n",
    "    .select([\n",
    "        'src_airport_id',\n",
    "        'dst_airport_id',\n",
    "    ])\n",
    "    # Group by source and count destinations\n",
    "    .groupBy('src_airport_id')\n",
    "    .agg({\n",
    "        'dst_airport_id': 'count',\n",
    "    })\n",
    "    .select(\n",
    "        F.col('src_airport_id').alias('airport_id'),\n",
    "        F.col('count(dst_airport_id)').alias('airport_size')\n",
    "    )\n",
    "    # Merge with airports to get airport name\n",
    "    .join(\n",
    "        airports\n",
    "        .select(\n",
    "            F.col('airport_id'),\n",
    "            F.col('airport').alias('airport_name')\n",
    "        ),\n",
    "        on='airport_id',\n",
    "        how='left',\n",
    "    )\n",
    "    # Select columns to be retained\n",
    "    .select(\n",
    "        F.col('airport_name'),\n",
    "        F.col('airport_size')\n",
    "    )\n",
    "    # Order by airport size descending\n",
    "    .orderBy('airport_size', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)\n",
    "counts.printSchema()\n",
    "show_as_html(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More complicated**\n",
    "\n",
    "- Defines multiple variables representing useful intermediate steps that are then combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute airport size using DataFrame API\n",
    "\n",
    "route_counts = (\n",
    "    routes\n",
    "    # Select only the columns that are needed\n",
    "    .select([\n",
    "        'src_airport_id',\n",
    "        'dst_airport_id',\n",
    "    ])\n",
    "    # Group by source and count destinations\n",
    "    .groupBy('src_airport_id')\n",
    "    .agg({\n",
    "        'dst_airport_id': 'count',\n",
    "    })\n",
    "    .select(\n",
    "        F.col('src_airport_id').alias('airport_id'),\n",
    "        F.col('count(dst_airport_id)').alias('airport_size')\n",
    "    )\n",
    ")\n",
    "\n",
    "airport_names = (\n",
    "    airports\n",
    "    .select(\n",
    "        F.col('airport_id'),\n",
    "        F.col('airport').alias('airport_name')\n",
    "    )\n",
    ")\n",
    "\n",
    "route_counts_with_airport_names = (\n",
    "    route_counts\n",
    "    # Merge with airports to get airport name\n",
    "    .join(\n",
    "        airport_names,\n",
    "        on='airport_id',\n",
    "        how='left',\n",
    "    )\n",
    "    # Select columns to be retained\n",
    "    .select(\n",
    "        F.col('airport_name'),\n",
    "        F.col('airport_size')\n",
    "    )\n",
    "    # Order by airport size descending\n",
    "    .orderBy('airport_size', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(route_counts)\n",
    "route_counts.printSchema()\n",
    "show_as_html(route_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airport_names)\n",
    "airport_names.printSchema()\n",
    "show_as_html(airport_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(route_counts_with_airport_names)\n",
    "route_counts_with_airport_names.printSchema()\n",
    "show_as_html(route_counts_with_airport_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
