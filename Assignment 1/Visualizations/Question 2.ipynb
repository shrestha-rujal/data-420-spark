{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0d871f-957f-4314-94e3-16b9c1c4cf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85832872-5393-4ea9-9f6c-6a49c94386ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/09/03 15:23:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>rsh224 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4040\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>rsh224 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>rsh224</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>8g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1756869817347</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>8g</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/rsh224/spark/</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1756869817477</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-d42b3243cb0b4526a4ba6abbd7b6e168</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>rsh224-notebook-4a1d61990d99f7d2</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>128</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>rsh224 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=8, executor_cores=4, worker_memory=8, master_memory=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787cdb5d-6b5a-4fe1-8cbe-5ef019d4537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c52349-b2c1-4ec5-805e-55f9f2fa7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d48877-7cbf-477c-9a31-957a1a3afaba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: double (nullable = true)\n",
      " |-- MEASUREMENT: string (nullable = true)\n",
      " |-- QUALITY: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      "\n",
      "DataFrame[ID: string, DATE: string, ELEMENT: string, VALUE: double, MEASUREMENT: string, QUALITY: string, SOURCE: string, TIME: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+------+-----------+-------+------+----+\n",
      "|ID         |DATE    |ELEMENT|VALUE |MEASUREMENT|QUALITY|SOURCE|TIME|\n",
      "+-----------+--------+-------+------+-----------+-------+------+----+\n",
      "|ASN00030019|20100101|PRCP   |24.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00030021|20100101|PRCP   |200.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00030022|20100101|TMAX   |294.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00030022|20100101|TMIN   |215.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00030022|20100101|PRCP   |408.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029121|20100101|PRCP   |820.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029126|20100101|TMAX   |371.0 |NULL       |NULL   |S     |NULL|\n",
      "|ASN00029126|20100101|TMIN   |225.0 |NULL       |NULL   |S     |NULL|\n",
      "|ASN00029126|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029126|20100101|TAVG   |298.0 |H          |NULL   |S     |NULL|\n",
      "|ASN00029127|20100101|TMAX   |371.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029127|20100101|TMIN   |225.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029127|20100101|PRCP   |8.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029129|20100101|PRCP   |174.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029130|20100101|PRCP   |86.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029131|20100101|PRCP   |56.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029132|20100101|PRCP   |800.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029136|20100101|PRCP   |22.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029137|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029139|20100101|TMAX   |298.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029139|20100101|TMIN   |270.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029139|20100101|PRCP   |30.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029141|20100101|TMAX   |360.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029141|20100101|TMIN   |249.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029141|20100101|PRCP   |12.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029150|20100101|PRCP   |84.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029152|20100101|PRCP   |560.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029154|20100101|PRCP   |420.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029161|20100101|PRCP   |40.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029163|20100101|PRCP   |70.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029167|20100101|TMAX   |334.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029167|20100101|TMIN   |242.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029167|20100101|PRCP   |166.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029069|20100101|PRCP   |40.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029077|20100101|TMAX   |292.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029077|20100101|TMIN   |259.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029077|20100101|PRCP   |12.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029077|20100101|TAVG   |265.0 |H          |NULL   |S     |NULL|\n",
      "|ASN00029080|20100101|PRCP   |70.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029081|20100101|PRCP   |120.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029086|20100101|PRCP   |220.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029089|20100101|PRCP   |140.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029090|20100101|PRCP   |206.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029100|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029011|20100101|PRCP   |1618.0|NULL       |NULL   |a     |NULL|\n",
      "|ASN00029012|20100101|TMAX   |310.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029012|20100101|TMIN   |239.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029012|20100101|PRCP   |86.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029014|20100101|PRCP   |320.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029015|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029016|20100101|PRCP   |50.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029027|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029030|20100101|PRCP   |42.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029031|20100101|PRCP   |490.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029032|20100101|PRCP   |320.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029036|20100101|PRCP   |78.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029037|20100101|PRCP   |70.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029038|20100101|TMAX   |335.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029038|20100101|TMIN   |247.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029038|20100101|PRCP   |34.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029039|20100101|TMAX   |297.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029039|20100101|TMIN   |254.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029039|20100101|PRCP   |116.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029047|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029048|20100101|PRCP   |165.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029051|20100101|PRCP   |480.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029052|20100101|PRCP   |18.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029054|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029058|20100101|TMAX   |356.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029058|20100101|TMIN   |228.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029058|20100101|PRCP   |66.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029061|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029063|20100101|TMAX   |312.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029063|20100101|TMIN   |244.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029063|20100101|PRCP   |158.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00029065|20100101|PRCP   |260.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027064|20100101|PRCP   |630.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027069|20100101|PRCP   |546.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027072|20100101|PRCP   |325.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027073|20100101|TMAX   |336.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027073|20100101|TMIN   |242.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027073|20100101|PRCP   |44.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027075|20100101|TMAX   |334.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027075|20100101|TMIN   |241.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00027075|20100101|PRCP   |298.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028000|20100101|PRCP   |224.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028004|20100101|TMAX   |335.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028004|20100101|TMIN   |248.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028004|20100101|PRCP   |130.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028007|20100101|TMAX   |348.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028007|20100101|TMIN   |250.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028007|20100101|PRCP   |25.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028008|20100101|TMAX   |315.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028008|20100101|TMIN   |248.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028008|20100101|PRCP   |276.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028013|20100101|PRCP   |0.0   |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028015|20100101|PRCP   |380.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028022|20100101|PRCP   |100.0 |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028023|20100101|PRCP   |12.0  |NULL       |NULL   |a     |NULL|\n",
      "|ASN00028031|20100101|PRCP   |30.0  |NULL       |NULL   |a     |NULL|\n",
      "+-----------+--------+-------+------+-----------+-------+------+----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a subset of the last year in daily into Spark from Azure Blob Storage using spark.read.csv\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType()),           # Character Station code\n",
    "    StructField(\"DATE\", StringType()),         # Date Observation date formatted as YYYYMMDD\n",
    "    StructField(\"ELEMENT\", StringType()),      # Character Element type indicator\n",
    "    StructField(\"VALUE\", DoubleType()),        # Real Data value for ELEMENT\n",
    "    StructField(\"MEASUREMENT\", StringType()),  # Character Measurement Flag\n",
    "    StructField(\"QUALITY\", StringType()),      # Character Quality Flag\n",
    "    StructField(\"SOURCE\", StringType()),       # Character Source Flag\n",
    "    StructField(\"TIME\", StringType()),         # Time Observation time formatted as HHMM\n",
    "])\n",
    "\n",
    "daily = spark.read.csv(\n",
    "    path=f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/daily/',\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "print(type(daily))\n",
    "daily.printSchema()\n",
    "print(daily)\n",
    "daily.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e788f883-07f4-4ed2-9ba1-a8a927090a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_enriched_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/stations-enriched'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22216d88-c0ce-415e-a18f-46acef329d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_enriched = spark.read.csv(stations_enriched_path, header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7faa4033-5d8f-4e12-962c-e7c19a7ae4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: double (nullable = true)\n",
      " |-- MEASUREMENT: string (nullable = true)\n",
      " |-- QUALITY: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a736d44a-366a-469a-b49b-048626c83382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- STATE_CODE: string (nullable = true)\n",
      " |-- COUNTRY_CODE: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- STATION_NAME: string (nullable = true)\n",
      " |-- GSN: string (nullable = true)\n",
      " |-- HCN_CRN_FLAG: string (nullable = true)\n",
      " |-- WMO_ID: string (nullable = true)\n",
      " |-- COUNTRY_NAME: string (nullable = true)\n",
      " |-- STATE_NAME: string (nullable = true)\n",
      " |-- ELEMENTS: string (nullable = true)\n",
      " |-- NUM_CORE_ELEMENTS: string (nullable = true)\n",
      " |-- NUM_OTHER_ELEMENTS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_enriched.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7ce637-2a93-457d-8d31-c73cc589e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_prcp = daily.filter(F.col('ELEMENT') == 'PRCP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7f7eae-b717-438c-ace5-8aae077912f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1084610240"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_prcp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0e5a07-d172-4302-9930-d21402cfdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_prcp.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2932835-b2b7-42b9-9df4-7305e897106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_country = daily_prcp.join(\n",
    "    stations_enriched,\n",
    "    how='inner',\n",
    "    on='ID'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b323e3-6008-499a-917d-b90cd7fc8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prcp_country.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa5fad3-2c51-4e20-bfa2-ae1556a43797",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_country = prcp_country.select('ID', 'DATE', F.col('VALUE').alias('PRCP'), 'COUNTRY_CODE', 'COUNTRY_NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e9ce0f-3045-44c5-8eb9-e28dc5ff0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prcp_country.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a83cc38-ab86-4618-aa22-019279e4b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_country = prcp_country.withColumn('DATE', F.to_date(F.col('DATE'), 'yyyyMMdd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa58924-6898-478f-9586-04dbbdc6a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_country = prcp_country.withColumn('DATE', F.date_trunc('year', 'DATE').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "900cd973-2774-49ec-998a-2b742f0b7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prcp_country.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52875e00-9273-45bc-98e0-e57ef8b84eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_prcp = (prcp_country\n",
    "                .where(F.col('PRCP') != -9999)\n",
    "                .withColumn('PRCP', F.col('PRCP') / 10.0)\n",
    "                .groupBy('DATE', 'COUNTRY_CODE', 'COUNTRY_NAME')\n",
    "                .agg(\n",
    "                    F.avg('PRCP').alias('AVG_PRCP'),\n",
    "                    F.count('*').alias('NUM_OBSERVATIONS'),\n",
    "                    F.countDistinct('ID').alias('NUM_STATIONS')\n",
    "                )\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab6c6a05-0b42-4446-8d5e-7d2657fc0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_prcp.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75a7a026-25b3-4c21-8cd8-40d85d07a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- COUNTRY_CODE: string (nullable = true)\n",
      " |-- COUNTRY_NAME: string (nullable = true)\n",
      " |-- AVG_PRCP: double (nullable = true)\n",
      " |-- NUM_OBSERVATIONS: long (nullable = false)\n",
      " |-- NUM_STATIONS: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_prcp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98dcd075-2122-47f8-bd7b-7a90ef91488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_year_country = grouped_prcp.withColumn('YEAR', F.year('DATE')).select('YEAR', 'COUNTRY_CODE', 'COUNTRY_NAME', 'AVG_PRCP', 'NUM_OBSERVATIONS', 'NUM_STATIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cc6a060-be68-4ff2-a4ff-5d052540bd29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prcp_year_country.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71be40b8-7b52-4f0e-bc42-7a5a53e04a26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prcp_year_country\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/rsh224/prcp_year_country\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_path' is not defined"
     ]
    }
   ],
   "source": [
    "prcp_year_country.write.mode('overwrite').option('header', True).csv(f'{output_path}/rsh224/prcp_year_country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604b099-65f1-4e5f-81cf-ce5a566cee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -ls {output_path}/rsh224/prcp_year_country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca6435-d174-4530-8fe9-7326f8172494",
   "metadata": {},
   "source": [
    "### Answer 2(a) second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3888e323-8a65-4b2d-bcae-489ccf36561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>(106 + 1) / 107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+--------------------+--------+-----------------+-----------------+\n",
      "|COUNT|START_YEAR|END_YEAR|            MIN_PRCP|MAX_PRCP|   AVG_PRCP_TOTAL|         STD_PRCP|\n",
      "+-----+----------+--------+--------------------+--------+-----------------+-----------------+\n",
      "|17726|      1750|    2025|-0.10767123287671247|   436.1|4.190457738064541|8.415885638536873|\n",
      "+-----+----------+--------+--------------------+--------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "prcp_year_country.agg(\n",
    "    F.count('*').alias('COUNT'),\n",
    "    F.min('YEAR').alias('START_YEAR'),\n",
    "    F.max('YEAR').alias('END_YEAR'),\n",
    "    F.min('AVG_PRCP').alias('MIN_PRCP'),\n",
    "    F.max('AVG_PRCP').alias('MAX_PRCP'),\n",
    "    F.avg('AVG_PRCP').alias('AVG_PRCP_TOTAL'),\n",
    "    F.stddev('AVG_PRCP').alias('STD_PRCP')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4d890-5162-41bd-a47f-a3092c8d254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_prcp.select('AVG_PRCP').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a8fc9-6b93-4efc-bcee-79ad7e042a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prcp_year_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2509bece-de82-4134-b836-24f92858d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.              (0 + 32) / 107]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprcp_year_country\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNUM_OBSERVATIONS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAVG_PRCP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:976\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    969\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    970\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         },\n\u001b[1;32m    974\u001b[0m     )\n\u001b[0;32m--> 976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prcp_year_country.filter(F.col('NUM_OBSERVATIONS') > 300).orderBy(F.desc('AVG_PRCP')).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1a62f48-6381-4bed-bad2-8ac176109a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_2024 = prcp_year_country.filter(F.col('YEAR') == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7abc3-dc0f-4aa5-a33e-3788a0aa7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_2024.orderBy(F.desc('AVG_PRCP')).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c67c127b-5aa0-41da-9415-8bf5279ab3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pdf = prcp_2024.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "073d0d3b-3b6a-4ea9-a951-c843f4c6bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1857ad9-9e85-49ea-b9a1-97b41588fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7263/1403283276.py:2: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n",
      "  world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))[[\"iso_a3\",\"name\",\"geometry\"]]\n"
     ]
    }
   ],
   "source": [
    "# World geometries\n",
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))[[\"iso_a3\",\"name\",\"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "103b51b3-db1a-438c-953c-e4aa7570d3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_a3</th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FJI</td>\n",
       "      <td>Fiji</td>\n",
       "      <td>MULTIPOLYGON (((180.00000 -16.06713, 180.00000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TZA</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESH</td>\n",
       "      <td>W. Sahara</td>\n",
       "      <td>POLYGON ((-8.66559 27.65643, -8.66512 27.58948...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>MULTIPOLYGON (((-122.84000 49.00000, -122.9742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USA</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>MULTIPOLYGON (((-122.84000 49.00000, -120.0000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iso_a3                      name  \\\n",
       "0    FJI                      Fiji   \n",
       "1    TZA                  Tanzania   \n",
       "2    ESH                 W. Sahara   \n",
       "3    CAN                    Canada   \n",
       "4    USA  United States of America   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((180.00000 -16.06713, 180.00000...  \n",
       "1  POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...  \n",
       "2  POLYGON ((-8.66559 27.65643, -8.66512 27.58948...  \n",
       "3  MULTIPOLYGON (((-122.84000 49.00000, -122.9742...  \n",
       "4  MULTIPOLYGON (((-122.84000 49.00000, -120.0000...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5649a64d-842a-4bd1-a8f4-28626f671f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "WORLD\n"
     ]
    }
   ],
   "source": [
    "# Map 2-letter -> 3-letter country codes for a reliable join\n",
    "# If pycountry isn't available, you can join by name as a fallback (see comment below).\n",
    "try:    \n",
    "    print('here')\n",
    "    def a2_to_a3(a2):\n",
    "        try:\n",
    "            return pycountry.countries.get(alpha_2=a2).alpha_3\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    pdf[\"iso_a3\"] = pdf[\"COUNTRY_CODE\"].apply(a2_to_a3)\n",
    "    \n",
    "    pdf[\"iso_a3\"] = pdf.apply(lambda r: fixes.get(r[\"COUNTRY_CODE\"], r[\"iso_a3\"]), axis=1)\n",
    "    world_join = world.merge(pdf, on=\"iso_a3\", how=\"left\")\n",
    "except Exception:    \n",
    "    print('WORLD')\n",
    "    # Fallback: join by cleaned names (expect more mismatches)\n",
    "    world_join = world.merge(pdf, left_on=\"name\", right_on=\"COUNTRY_NAME\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b9958-0053-40bd-8553-849a625e2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust color limits (avoid outliers skewing the map)\n",
    "vmin = np.nanpercentile(world_join[\"AVG_PRCP\"], 2)\n",
    "vmax = np.nanpercentile(world_join[\"AVG_PRCP\"], 98)\n",
    "\n",
    "# Reproject and plot\n",
    "world_robin = world_join.to_crs(\"ESRI:54030\")  # Robinson projection\n",
    "ax = world_robin.plot(\n",
    "    column=\"AVG_PRCP\",\n",
    "    cmap=\"Blues\",\n",
    "    vmin=vmin, vmax=vmax,\n",
    "    legend=True,\n",
    "    linewidth=0.2, edgecolor=\"white\",\n",
    "    missing_kwds={\"color\": \"lightgrey\", \"edgecolor\": \"white\", \"hatch\": \"///\", \"label\": \"No data\"},\n",
    "    figsize=(11.69, 8.27),  # A4 landscape\n",
    ")\n",
    "\n",
    "ax.set_title(\"Average daily rainfall (mm) in 2024 — country means (station-balanced)\", fontsize=12, pad=8)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc720934-6366-4e9a-91b7-9dc28f090798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1752fa-aeec-48d1-b91b-f42283ed8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3e37f-8c80-4387-b875-35d459d6a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_states_count_path = f'{output_path}/rsh224/states_count_stations'\n",
    "states_enriched.write.mode('overwrite').option('header', True).csv(output_states_count_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2f575-4974-40e8-8c5e-dc8e3c1cf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {output_path}/rsh224/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0484b39-2b75-4af8-a1de-b1a7a2d19e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
